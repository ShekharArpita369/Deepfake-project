# -*- coding: utf-8 -*-
"""Deepseek.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1zLKHomgtxbldyUZe6ucYKucweVLGDQCc
"""

First, mount Google Drive
from google.colab import drive
drive.mount('/content/drive')

# Import required libraries
import tensorflow as tf
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Conv2D, MaxPooling2D, Dense, Flatten, Dropout
from tensorflow.keras.preprocessing.image import ImageDataGenerator
import numpy as np
import matplotlib.pyplot as plt
import os

#Set your dataset path (update this to your Google Drive path)
DATASET_PATH = '/content/drive/MyDrive/Dataset'  # Contains 'real' and 'fake' folders
real_images_path = os.path.join(DATASET_PATH, 'real')
fake_images_path = os.path.join(DATASET_PATH, 'fake')

# Create data generators with augmentation
train_datagen = ImageDataGenerator(
    rescale=1/255,
    rotation_range=20,
    width_shift_range=0.2,
    height_shift_range=0.2,
    horizontal_flip=True,
    validation_split=0.2
)

!ls "/content/drive/MyDrive/Colab Notebooks/Dataset_Deepfake"

DATASET_PATH = '/content/drive/MyDrive/Colab Notebooks/Dataset_Deepfake/Dataset'

# Create training generator
train_generator = train_datagen.flow_from_directory(
    DATASET_PATH,
    target_size=(150, 150),
    batch_size=32,
    class_mode='binary',
    subset='training'
)

# Create training generator
train_generator = train_datagen.flow_from_directory(
    DATASET_PATH,
    target_size=(150, 150),
    batch_size=32,
    class_mode='binary',
    subset='training'
)

# Build the model . We use CNN model for image Classification
model = Sequential([
    Conv2D(32, (3, 3), activation='relu', input_shape=(150, 150, 3)),
    MaxPooling2D(2, 2),
    Conv2D(64, (3, 3), activation='relu'),
    MaxPooling2D(2, 2),
    Conv2D(64, (3, 3), activation='relu'),
    MaxPooling2D(2, 2),
    Flatten(),
    Dense(128, activation='relu'),
    Dropout(0.5),
    Dense(1, activation='sigmoid')
])

# Compile the model
model.compile(
    optimizer='adam',
    loss='binary_crossentropy',
    metrics=['accuracy']
)

history = model.fit(train_generator, validation_data=validation_generator, epochs=10)

# Create validation generator
validation_generator = train_datagen.flow_from_directory(
    DATASET_PATH,
    target_size=(150, 150),
    batch_size=32,
    class_mode='binary',
    subset='validation'
)

# Save the model
model.save('/content/drive/MyDrive/deepfake_model.h5')

# Function to predict and display results
def predict_image(image_path):
    # Load and preprocess the image
    img = tf.keras.preprocessing.image.load_img(
        image_path,
        target_size=(150, 150)
    )
    img_array = tf.keras.preprocessing.image.img_to_array(img)
    img_array = img_array / 255.0
    img_array = np.expand_dims(img_array, axis=0)
    # Make prediction
    prediction = model.predict(img_array)[0][0]
    # Create figure for visualization
    plt.figure(figsize=(10, 5))

    # Display original image
    plt.subplot(1, 2, 1)
    plt.imshow(img)
    plt.title('Input Image')
    plt.axis('off')
    # Display image with prediction
    plt.subplot(1, 2, 2)
    plt.imshow(img)

    # Set color and label based on prediction
    is_fake = prediction > 0.5
    color = 'red' if is_fake else 'green'
    label = 'FAKE' if is_fake else 'REAL'
    confidence = prediction if is_fake else (1 - prediction)
    # Add colored border
    plt.gca().spines['top'].set_color(color)
    plt.gca().spines['bottom'].set_color(color)
    plt.gca().spines['left'].set_color(color)
    plt.gca().spines['right'].set_color(color)
    plt.gca().spines['top'].set_linewidth(5)
    plt.gca().spines['bottom'].set_linewidth(5)
    plt.gca().spines['left'].set_linewidth(5)
    plt.gca().spines['right'].set_linewidth(5)

    plt.title(f'{label}\nConfidence: {confidence:.2%}', color=color, pad=20)
    plt.axis('off')

    plt.tight_layout()
    plt.show()

    return label, confidence

# Function to plot training history
def plot_training_history(history):
    plt.figure(figsize=(12, 4))

    # Plot accuracy
    plt.subplot(1, 2, 1)
    plt.plot(history.history['accuracy'], label='Training Accuracy')
    plt.plot(history.history['val_accuracy'], label='Validation Accuracy')
    plt.title('Model Accuracy')
    plt.xlabel('Epoch')
    plt.ylabel('Accuracy')
    plt.legend()

    # Plot loss
    plt.subplot(1, 2, 2)
    plt.plot(history.history['loss'], label='Training Loss')
    plt.plot(history.history['val_loss'], label='Validation Loss')
    plt.title('Model Loss')
    plt.xlabel('Epoch')
    plt.ylabel('Loss')
    plt.legend()

    plt.tight_layout()
    plt.show()

history.history.keys()

plot_training_history(history)

from tensorflow.keras.applications import VGG16

# Load VGG16 with ImageNet weights
base_model = VGG16(weights='imagenet', include_top=False, input_shape=(150, 150, 3))
base_model.trainable = False  # Freeze the base model

# Build your model
model = Sequential([
    base_model,
    Flatten(),
    Dense(128, activation='relu'),
    Dropout(0.5),
    Dense(1, activation='sigmoid')
])

model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])

from tensorflow.keras.callbacks import EarlyStopping

early_stopping = EarlyStopping(monitor='val_loss', patience=3, restore_best_weights=True)

history = model.fit(
    train_generator,
    epochs=3,
    validation_data=validation_generator,
    callbacks=[early_stopping]
)

from sklearn.metrics import confusion_matrix, classification_report

# Assuming you have a test set or validation set
predictions = model.predict(validation_generator)
predicted_classes = (predictions > 0.5).astype("int32")
true_classes = validation_generator.classes

# Confusion Matrix
cm = confusion_matrix(true_classes, predicted_classes)
print(cm)

# Classification Report
print(classification_report(true_classes, predicted_classes))

# Example usage for prediction
# Replace with your test image path
test_image_path = '/content/drive/MyDrive/Colab Notebooks/Dataset_Deepfake/Dataset/train_fake/mid_84_1111.jpg'
label, confidence = predict_image(test_image_path)
print(f"Prediction: {label}")
print(f"Confidence: {confidence:.2%}")

from sklearn.metrics import confusion_matrix, classification_report
import matplotlib.pyplot as plt
import seaborn as sns

# Assuming you have a test set or validation set
predictions = model.predict(validation_generator)
predicted_classes = (predictions > 0.5).astype("int32")
true_classes = validation_generator.classes

# Confusion Matrix
cm = confusion_matrix(true_classes, predicted_classes)

# Plotting the Confusion Matrix
plt.figure(figsize=(8, 6))
sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=['Real', 'Fake'], yticklabels=['Real', 'Fake'])
plt.ylabel('Actual')
plt.xlabel('Predicted')
plt.title('Confusion Matrix')
plt.show()

# Classification Report
report = classification_report(true_classes, predicted_classes, target_names=['Real', 'Fake'])
print(report)

model.save('/content/drive/MyDrive/deepfake_model.h5')
print("âœ… Model saved!")

